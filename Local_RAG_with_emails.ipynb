{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiq/llm/blob/main/Local_RAG_with_emails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook illustrates how to build **local** RAG with your emails.\n",
        "Download it as a Jupyter notebook and run **locally**.\n",
        "\n",
        "For privacy reasons, email examples are not provided. Feel free to use your own :)\n",
        "\n",
        "\n",
        "Built with:\n",
        "- [Unstructured.io](https://unstructured.io/)\n",
        "- [LangChain](https://www.langchain.com/)\n",
        "- [Ollama](https://ollama.com/)"
      ],
      "metadata": {
        "id": "PCAfep-YAX8A"
      },
      "id": "PCAfep-YAX8A"
    },
    {
      "metadata": {
        "id": "2b2ebf7757be6506"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Setup step 1: download ollama, and pull the models\n",
        "# !ollama pull llama3\n",
        "# !ollama pull nomic-embed-text"
      ],
      "id": "2b2ebf7757be6506"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "# Setup step 2: install libraries\n",
        "# !pip install -q langchain unstructured[all-docs] faiss-cpu"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-29T13:00:08.527891Z",
          "start_time": "2024-04-29T13:00:07.297160Z"
        },
        "id": "86e917742f6d7270"
      },
      "cell_type": "code",
      "source": [
        "#Step 2: Preprocess emails with partition_email from Unstructured, turning them into elements.\n",
        "\n",
        "import os\n",
        "from unstructured.partition.email import partition_email\n",
        "\n",
        "def preprocess_emails(directory):\n",
        "  elements = []\n",
        "  for root, _, files in os.walk(directory):\n",
        "    for file in files:\n",
        "        if file.endswith(\".eml\"):\n",
        "            elems = partition_email(filename=os.path.join(root,\n",
        "                                                          file))\n",
        "            elements.extend(elems)\n",
        "  return elements\n",
        "\n",
        "email_elements = preprocess_emails(\"emails\")"
      ],
      "id": "86e917742f6d7270",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-29T13:01:59.504987Z",
          "start_time": "2024-04-29T13:01:58.401694Z"
        },
        "id": "f5a68771a12d16e6"
      },
      "cell_type": "code",
      "source": [
        "# Step 3: Chunk the email elements and prepare them for LangChain.\n",
        "# `chunk_by_title` will take into account the documents' logical structure for better RAG results.\n",
        "\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "chunked_elements = chunk_by_title(email_elements)\n",
        "\n",
        "documents = []\n",
        "for element in chunked_elements:\n",
        "    metadata = element.metadata.to_dict()\n",
        "    documents.append(Document(page_content=element.text,\n",
        "                              metadata=metadata))"
      ],
      "id": "f5a68771a12d16e6",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-29T13:03:14.971322Z",
          "start_time": "2024-04-29T13:02:22.350802Z"
        },
        "id": "5d94893079c73ae7",
        "outputId": "477361b9-f94d-4c43-9a39-d87123cbb8f4"
      },
      "cell_type": "code",
      "source": [
        "# Step4: Create vector storage with embeddings, prepare the retriever\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "db = FAISS.from_documents(documents, OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True))\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ],
      "id": "5d94893079c73ae7",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 3116/3116 [00:52<00:00, 59.47it/s]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-29T13:05:01.106618Z",
          "start_time": "2024-04-29T13:05:01.014006Z"
        },
        "id": "20ea6985b6f83a98"
      },
      "cell_type": "code",
      "source": [
        "# Step 5: Set up the local model:\n",
        "\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "local_model = \"llama3\"\n",
        "llm = ChatOllama(model=local_model, num_predict=400,\n",
        "                 stop=[\"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"])"
      ],
      "id": "20ea6985b6f83a98",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-29T13:10:40.236227Z",
          "start_time": "2024-04-29T13:10:40.231606Z"
        },
        "id": "e2bf24dc75f86061"
      },
      "cell_type": "code",
      "source": [
        "# Step 6: Set up the RAG chain:\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "Answer the user's question using provided context. Stick to the facts, do not draw your own conclusions.\n",
        "Question: {question}\n",
        "Context: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "id": "e2bf24dc75f86061",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-29T13:10:41.036600Z",
          "start_time": "2024-04-29T13:10:41.034082Z"
        },
        "id": "6c605e31b964172f"
      },
      "cell_type": "code",
      "source": [
        "question = \"What can you tell me about Cohere's Command+ models?\""
      ],
      "id": "6c605e31b964172f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-29T13:10:45.566881Z",
          "start_time": "2024-04-29T13:10:42.724386Z"
        },
        "id": "15e540ef9cb06e86",
        "outputId": "d3b6c291-af78-48f0-9eaa-2f1db2312d3a"
      },
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(question)"
      ],
      "id": "15e540ef9cb06e86",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 24.45it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"According to the provided context, Cohere's Command+ models are available with open weight access, meaning you can use them without restrictions or costs for non-commercial purposes. However, these models are not commercially applicable, and their weights are available for non-commercial use only. The training data used to develop these models is not shared.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}